---
layout: post
title: "STA303, Maximum Likelihood"
date: 2021-02-04
---

While we are waiting in line, I think I can explain maximum likelihood estimation to you. Remember least square estimation? We use it to find the beast fit for a set of data points by minimizing the sum of residuals. In maximum likelihood estimation, we are not trying to minimize the sum of residuals, but to maximize the probability of producing our observed data. A likelihood is a function that tells us how likely we are to observe our data for a given parameter value. Once the likelihood function is derived, maximum likelihood estimation is nothing more than a simple optimization problem. So, treating Maximum likelihood estimation as an optimization problem, where we look for parameters that results in the best fit for the joint probability of the data sample
Maximum likelihood estimation depends on the assumption of model and the likelihood function. It the assumptions are correct, the maximum likelihood estimator is the most efficient. 

Just remember the differences between least square estimation and maximum likelihood estimation. One is minimizing the sum of residuals; one is maximizing the probability. Looks like itâ€™s almost our turn. Come to my office hour is you still have question. See you!
